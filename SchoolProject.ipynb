{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: find users that go to the school by looking for the hashtags or words:\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "***Cells below are from Week2-TwitterMining workbook and have not been formated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "def oauth_login():\n",
    "    # XXX: Go to  to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://developer.twitter.com/en/docs/basics/authentication/overview/oauth\n",
    "    # for more information on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = ''\n",
    "    CONSUMER_SECRET = ''\n",
    "    OAUTH_TOKEN = ''\n",
    "    OAUTH_TOKEN_SECRET = ''\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "# Sample usage\n",
    "twitter_api = oauth_login()    \n",
    "\n",
    "# Nothing to see by displaying twitter_api except that it's now a\n",
    "# defined variable\n",
    "\n",
    "print(twitter_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "\n",
    "    # See https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n",
    "    # and https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators\n",
    "    # for details on advanced search criteria that may be useful for \n",
    "    # keyword arguments\n",
    "    \n",
    "    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://developer.twitter.com/en/docs/basics/rate-limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    # Read this https://developer.twitter.com/en/docs/tweets/timelines/guides/working-with-timelines to understand why we use \n",
    "    # max_id to populate tweets\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "q = \"Game of Thrones\"\n",
    "# q=\"from:Cristiano\"\n",
    "results = twitter_search(twitter_api, q, max_results=1000)\n",
    "        \n",
    "# Show one sample search result by slicing the list...\n",
    "print(json.dumps(results[0], indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding topics of interest by using the filtering capabilities it offers.\n",
    "\n",
    "import sys\n",
    "import twitter\n",
    "\n",
    "# Query terms\n",
    "\n",
    "q = 'WednesdayWisdom' # Comma-separated list of terms\n",
    "\n",
    "print('Filtering the public timeline for track={0}'.format(q), file=sys.stderr)\n",
    "sys.stderr.flush()\n",
    "\n",
    "# Returns an instance of twitter.Twitter\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# Reference the self.auth parameter\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "\n",
    "# See https://developer.twitter.com/en/docs/tweets/filter-realtime/guides/basic-stream-parameters.html\n",
    "stream = twitter_stream.statuses.filter(track=q)\n",
    "for tweet in stream:\n",
    "    print(tweet['text'])\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_texts = [ status['text']\n",
    "                 for status in results ]\n",
    "\n",
    "screen_names = [ user_mention['screen_name']\n",
    "                 for status in results\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "hashtags = [ hashtag['text']\n",
    "             for status in results\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w\n",
    "          for t in status_texts\n",
    "              for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print(json.dumps(status_texts[0:5], indent=1))\n",
    "print(json.dumps(screen_names[0:5], indent=1))\n",
    "print(json.dumps(hashtags[0:5], indent=1))\n",
    "print(json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: run sentiment analysis on the tweets included in the hashtags\n",
    "\n",
    "***Cells below oar from Week3a-TwitterMining-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_api = oauth_login()\n",
    "\n",
    "# Reference the self.auth parameter\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "iterator = twitter_stream.statuses.sample()\n",
    "\n",
    "tweets = []\n",
    "for tweet in iterator:\n",
    "    try:\n",
    "        if tweet['lang'] == 'en':\n",
    "            tweets.append(tweet)\n",
    "    except:\n",
    "        pass\n",
    "    if len(tweets) == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(len(tweets))\n",
    "\n",
    "for i, t in enumerate(tweets):\n",
    "    # Extract the text portion of the tweet\n",
    "    text = t['text']\n",
    "    \n",
    "    # Measure the polarity of the tweet\n",
    "    polarity = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Store the normalized, weighted composite score\n",
    "    scores[i] = polarity['compound']\n",
    "\n",
    "most_positive = np.argmax(scores)\n",
    "most_negative = np.argmin(scores)\n",
    "\n",
    "print('{0:6.3f} : \"{1}\"'.format(scores[most_positive], tweets[most_positive]['text']))\n",
    "\n",
    "print('{0:6.3f} : \"{1}\"'.format(scores[most_negative], tweets[most_negative]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Store user, tweet, and sentiment analysis score above () and below () in a doc or database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Create word maps of the words in the tweets that cross the threashhold\n",
    "\n",
    "***Cells below are from Week2-TwitterMining workbook and have not been formated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "words\n",
    "wc = wordcloud.WordCloud(stopwords=['https','RT','CO'])\n",
    "clean_string = ','.join(words)\n",
    "wc.generate(clean_string)\n",
    "plt.imshow(wc.recolor( random_state=3))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Create a network of the users bc of their followers/who they follow\n",
    "\n",
    "-would have to account for users who have tweeted multiple times, should we do this when saving the sentiment analysis/user/tweet so users can have multiple tweets or would we sort through when creating the network to find users with multiple tweets\n",
    "\n",
    "***Cells below are from Networkx_twitter and have not been formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('./data/friends/list.PyTennessee.json')\n",
    "\n",
    "data = json.load(f)\n",
    "pairs = []\n",
    "\n",
    "for user in data['users']:\n",
    "    pairs.append(('PyTennessee', str(user['screen_name'])))\n",
    "\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the relationship data is split across files, we need to\n",
    "# walk through all of them to get the data.\n",
    "import os\n",
    "\n",
    "for (dir_path, dir_names, file_names) in os.walk('./data/friend_relationships/'):\n",
    "    files = file_names\n",
    "    \n",
    "for file_name in files:\n",
    "    with open('./data/friend_relationships/' + file_name) as p:\n",
    "        pair_data = json.load(p)\n",
    "        for k in pair_data.keys():\n",
    "            twitter_pair = k.split()\n",
    "            if pair_data[k]['relationship']['source']['following'] is True:\n",
    "                pairs.append((str(twitter_pair[0]), str(twitter_pair[1])))\n",
    "            elif pair_data[k]['relationship']['source']['followed_by'] is True:\n",
    "                pairs.append((str(twitter_pair[1]), str(twitter_pair[0])))\n",
    "                \n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import operator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an undirected graph. What's going on?\n",
    "g = nx.Graph()\n",
    "g.add_edges_from(pairs)\n",
    "nx.draw_networkx(g,font_size=6)\n",
    "plt.axis('off')\n",
    "plt.savefig('undirected.png', dpi=500)\n",
    "# How connected is the network?\n",
    "# Very connected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: if you want to sort a dictionary to easily \n",
    "# find the highest and lowest values, use this function \n",
    "# on the output of the centrality measures like degree_centrality():\n",
    "\n",
    "import operator\n",
    "\n",
    "def centrality_sort(centrality_dict):\n",
    "    return sorted(centrality_dict.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# ex. degree_sorted = centrality_sort(degree_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest degree centrality?\n",
    "degree_centrality = nx.degree_centrality(g)\n",
    "degree_sorted = centrality_sort(degree_centrality)\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest degree:', degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest degree:', degree_sorted[:5])\n",
    "print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest betweenness centrality?\n",
    "betweenness = nx.betweenness_centrality(g)\n",
    "betweenness_sorted = centrality_sort(betweenness)\n",
    "print ('-------------Betweenness Centrality-------------')\n",
    "print ('Highest betweenness:', betweenness_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest betweenness:', betweenness_sorted[:5])\n",
    "print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest closeness centrality?\n",
    "closeness = nx.closeness_centrality(g)\n",
    "closeness_sorted = centrality_sort(closeness)\n",
    "print ('-------------Closeness Centrality-------------')\n",
    "print ('Highest closeness:', closeness_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest closeness:', closeness_sorted[:5])\n",
    "\n",
    "# At the end, discuss these questions more in-depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at subsections of the graph.\n",
    "\n",
    "# Top 20 highest in-degree centrality scores:\n",
    "highest_degree = [node[0] for node in degree_sorted[-20:]]\n",
    "sub = g.subgraph(highest_degree)\n",
    "nx.draw_networkx(sub)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_degree = [node[0] for node in degree_sorted[:20]]\n",
    "subl = g.subgraph(lowest_degree)\n",
    "nx.draw_networkx(subl)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed graph\n",
    "\n",
    "d = nx.DiGraph()\n",
    "\n",
    "d.add_edges_from(pairs)\n",
    "nx.draw_networkx(d)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some degree centrality measures for directed graphs:\n",
    "# in_degree_centrality(): number of incoming connections (number of people following you)\n",
    "# out_degree_centrality(): number of outgoing connections (number of people you follow)\n",
    "in_degree_centrality = nx.in_degree_centrality(d)\n",
    "in_degree_sorted = sorted(in_degree_centrality.items(), key=operator.itemgetter(1))\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest in degree:', in_degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest in degree:', in_degree_sorted[:5])\n",
    "print ('\\n')\n",
    "\n",
    "out_degree_centrality = nx.out_degree_centrality(d)\n",
    "out_degree_sorted = sorted(out_degree_centrality.items(), key=operator.itemgetter(1))\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest out degree:', out_degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest out degree:', out_degree_sorted[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at subsections of the graph. Just like we did above.\n",
    "\n",
    "# Top 20 highest in-degree centrality scores:\n",
    "highest_in_degree = [node[0] for node in in_degree_sorted[-20:]]\n",
    "subin = d.subgraph(highest_in_degree)\n",
    "nx.draw_networkx(subin)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 highest out-degree centrality scores:\n",
    "highest_out_degree = [node[0] for node in out_degree_sorted[-20:]]\n",
    "subin = d.subgraph(highest_out_degree)\n",
    "nx.draw_networkx(subin)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# we need this 'magic' command to draw graphs inline\n",
    "%matplotlib inline  \n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "# let's attach a size attribute to each node to describe how big we want the node to be\n",
    "g.add_node(1, size= 800)\n",
    "g.add_node(2, size= 200)\n",
    "g.add_node(3, size= 200)\n",
    "g.add_node(4, size= 200)\n",
    "g.add_node(5, size= 200)\n",
    "\n",
    "g.add_edge(1,2, thickness= 20)\n",
    "g.add_edge(1,3, thickness= 20)\n",
    "g.add_edge(1,4, thickness= 20)\n",
    "g.add_edge(1,5, thickness= 20)\n",
    "g.add_edge(2,3, thickness= 5)\n",
    "g.add_edge(3,4, thickness= 5)\n",
    "g.add_edge(4,5, thickness= 5)\n",
    "g.add_edge(5,2, thickness= 5)\n",
    "\n",
    "# let's iterate through the nodes and edges and extract the list of node & edge sizes\n",
    "node_size = [attribs['size'] for (node, attribs) in g.nodes(data=True)]\n",
    "edge_thickness = [attribs['thickness'] for (v_from, v_to, attribs) in g.edges(data=True)]\n",
    "\n",
    "LIGHT_BLUE = '#A0CBE2'\n",
    "\n",
    "nx.draw_networkx(g, \n",
    "    node_size = node_size,   # node_size can either take a single value (where all nodes will be size N),\n",
    "                             # or a list of values, where Nth list value will be the size for the Nth node\n",
    "    width = edge_thickness,  # similarly, the Nth value corresponds to the width for edge N\n",
    "    node_color = LIGHT_BLUE,\n",
    "    edge_color = LIGHT_BLUE,\n",
    "    font_size = 15,\n",
    "    with_labels = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_txt = './data/retweets.txt'\n",
    "G = nx.read_edgelist(edgelist_txt, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYOUTS = {\n",
    "    'circular': nx.circular_layout,\n",
    "    'fr': nx.fruchterman_reingold_layout,\n",
    "    'random': nx.random_layout,\n",
    "    'shell': nx.shell_layout,\n",
    "    'spectral': nx.spectral_layout,\n",
    "    'spring': nx.spring_layout\n",
    "}\n",
    "\n",
    "def save_layout(G, layout_name):\n",
    "    elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] >1.5]\n",
    "    esmall=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] <=1.5]\n",
    "    nlarge=[n for n in G.nodes() if n in ['PyTennessee']]\n",
    "    pos=LAYOUTS[layout_name](G) # positions for all nodes\n",
    "\n",
    "    print (nlarge)\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G,pos,nodelist=nlarge,node_size=1)\n",
    "\n",
    "    # edges\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=elarge, width=1)\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=esmall, width=1,alpha=0.5,edge_color='#cccccc')\n",
    "\n",
    "    # labelsM\n",
    "    labels={}\n",
    "    labels['PyTennessee']='PyTennessee'\n",
    "    nx.draw_networkx_labels(G,pos,labels,font_size=6)\n",
    "    #nx.draw_networkx_labels(G,pos,nodelist=nlarge,font_size=6,font_family='sans-serif')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(layout_name + '.png', dpi=500)\n",
    "\n",
    "save_layout(G, 'spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Visually identify the users that have tweeted a negative tweet or a positive tweet with red or green"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
