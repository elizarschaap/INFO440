{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Twitter Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<twitter.api.Twitter object at 0x1a1a52da20>\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "import json\n",
    "\n",
    "def oauth_login():\n",
    "    # XXX: Go to  to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://developer.twitter.com/en/docs/basics/authentication/overview/oauth\n",
    "    # for more information on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = 'ldJnCzzuCM3Col1pMcmSyVaDT'\n",
    "    CONSUMER_SECRET = 'RWCNFVehlhAHzarhuAJA7vjSfoQx1FB96HLY7VR2U1d6mDj31w'\n",
    "    OAUTH_TOKEN = '1112769260818972672-FFlTewXnvPcbcJAM04vYqk8DPxjYcG'\n",
    "    OAUTH_TOKEN_SECRET = 'D5EuZziAVsW26tHuZCGYRw2uiKPdfdUgaaIszGatEsdB6'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "# Sample usage\n",
    "twitter_api = oauth_login()    \n",
    "\n",
    "print(twitter_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combing Twitter for Pennsbury High School related Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @levittownnow: Would you be interested in a hard cover or soft cover book featuring more than 100 curated photos from Pennsbury High Sch…\n",
      "Would you be interested in a hard cover or soft cover book featuring more than 100 curated photos from Pennsbury Hi… https://t.co/BWJKRFex5J\n",
      "RT @WBCBNews: Pennsbury High School’s senior prom was back and better than ever Saturday night.\n",
      "\n",
      "https://t.co/ZQ91aCDGrw\n",
      "Pennsbury High School’s senior prom was back and better than ever Saturday night.\n",
      "\n",
      "https://t.co/ZQ91aCDGrw\n",
      "RT @thermoHailine: Had a great time talking to the Pennsbury High School seniors about the ocean and climate change (and was only mistaken…\n",
      "Had a great time talking to the Pennsbury High School seniors about the ocean and climate change (and was only mist… https://t.co/B6St5BgJfc\n",
      "THE BEST PROM EVAH! At #Pennsbury High School, Desiigner and DJ Pauly D rock 'the Best Prom in America'  https://t.co/hZaIjK5Lro\n",
      "THE BEST PROM EVAH! At #Pennsbury High School, Desiigner and DJ Pauly D rock 'the Best Prom in America'  https://t.co/VNaJfzDSOH\n",
      "RT @DJPaulyD: I Love Prom This Time A Yearrrrr!!!! 🙌🏽🙌🏽 #BestPromInAmerica #prom2019🎓 U Guys Rock!! @ Pennsbury High School https://t.co/lS…\n",
      "RT @EPAFootball: Player Previews in 100 Days – Day 16: Drew Hensor, Pennsbury (1) https://t.co/eZeRzQRyCA @DrewHensor @pennsburyfb @pnsbrya…\n",
      "before vs. after the desiigner disaster @ Pennsbury High School https://t.co/RlRKSpAK4T\n",
      "Desiigner and Pauly D being at Pennsbury prom makes me miss high school just ~a little~ bit\n",
      "RT @PatrickEddis: Tonight is the #Pennsbury High School #SeniorProm, which I attended *many* years ago.\n",
      "\n",
      "How to describe it? Kinda like a m…\n",
      "I Love Prom This Time A Yearrrrr!!!! 🙌🏽🙌🏽 #BestPromInAmerica #prom2019🎓 U Guys Rock!! @ Pennsbury High School https://t.co/lSy7czuMqF\n",
      "@CorbinRadul316 @DJPaulyD This was at Pennsbury high school prom in Fairless Hills PA. Pennsbury has the best senior prom.\n",
      "Float Fun #pennsburyprom 2019 w/ jack_kittle m.akayla7 elliemay_004 its_rowan.k kerryanne_kittle @ Pennsbury High S… https://t.co/laOjCtEWpX\n",
      "The Wall at #pennsburyprom 2019 @ Pennsbury High School https://t.co/NP4uV74Psn\n",
      "Tonight is the #Pennsbury High School #SeniorProm, which I attended *many* years ago.\n",
      "\n",
      "How to describe it? Kinda li… https://t.co/4w464vkuuU\n",
      "@pnbrock next year you have to perform at Pennsbury high school prom we have the best prom in America you can look… https://t.co/Hii9ClsPKu\n",
      "Photos: #Pennsbury High School seniors arrive for \"America's Best Prom\"  \n",
      "https://t.co/X53kqHklOc\n",
      "Nicki is ready for her Sr Prom! Just gorgeous!have fun, Nicki! I love you so much! #pennsburyprom2019 @ Pennsbury H… https://t.co/dn9kUpaLBq\n",
      "A gym transformed to celebrate music legends #pennsburyprom @ Pennsbury High School https://t.co/Ra0KWbCp4p\n",
      "A bathroom transformed for #pennsburyprom #musicofthedecades @ Pennsbury High School https://t.co/YynAC8Iwle\n",
      "#pennsburyprom tour—a school transformed part 1 @ Pennsbury High School https://t.co/CsOqRjVqSi\n",
      "PROM TO THE MUSIC: Rap singer Desiigner, DJ Pauly D to headline the Pennsbury High School Senior Prom - Bucks Local… https://t.co/baPAy8zaCn\n",
      "PROM TO THE MUSIC: Rap singer Desiigner, DJ Pauly D to headline the Pennsbury High School Senior Prom - Bucks Local… https://t.co/uzvTtVKzlg\n",
      "My High School! PROM TO THE MUSIC: Pennsbury High School's Class of 2019 preps for the 'Best Prom in America'… https://t.co/E30TzhEfWh\n",
      "Player Previews in 100 Days – Day 16: Drew Hensor, Pennsbury (1) https://t.co/eZeRzQRyCA @DrewHensor @pennsburyfb @pnsbryathletics\n",
      "🚨BREAKING🚨3 Years In A Row Pennsbury High School Will Have World Touring Dj Pauly D As It’s Headliner For It’s Seni… https://t.co/e5VxgWT55E\n",
      "RT @FallsTwpPolice: Please help this male terrier get home!   He was found by Pennsbury High School on May 28th.  Please contact the Bucks…\n",
      "PROM TO THE MUSIC: Pennsbury High School's Class of 2019 preps for the 'Best Prom in America' - Bucks Local News: https://t.co/IXXxZPF6Z0\n",
      "First cousins Justin Massielo and Emelie Curtis, both #Pennsbury High School athletes, both reached impressive mile… https://t.co/KrIkraCfGC\n",
      "@TheEllenShow Stick around PA and head up to The Best Prom in America in Fairless Hills.  Pennsbury HS is about to… https://t.co/n4rnkZwMuh\n",
      "Please help this male terrier get home!   He was found by Pennsbury High School on May 28th.  Please contact the Bu… https://t.co/eAG9VdK0g6\n",
      "PROM TO THE MUSIC: #Pennsbury High School's Class of 2019 preps for the 'Best Prom in America' https://t.co/gyyuL5FwgM\n",
      "PROM TO THE MUSIC: #Pennsbury High School's Class of 2019 preps for the 'Best Prom in America'  https://t.co/waWv9CnXF9\n",
      "Pennsbury High School, my alma mater (class of 1992) has one of the best proms in the country! Held in a transforme… https://t.co/eKTTx1iAxT\n",
      "@DavidDobrik Please pop out to the Best Prom in America on June 1st @ Pennsbury High School! @DrakeBell  performed… https://t.co/8DugaeogdF\n",
      "@DavidDobrik The Best Prom in America is happening THIS Saturday! Don’t believe me? Ask Drake Bell, Maroon 5, John… https://t.co/hGxauIe1fz\n"
     ]
    }
   ],
   "source": [
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://developer.twitter.com/en/docs/basics/rate-limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    # Read this https://developer.twitter.com/en/docs/tweets/timelines/guides/working-with-timelines to understand why we use \n",
    "    # max_id to populate tweets\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "q = \"Pennsbury High School\"\n",
    "results = twitter_search(twitter_api, q, max_results=1000)\n",
    "\n",
    "        \n",
    "tweets = []\n",
    "\n",
    "for i, tweet in enumerate(results):\n",
    "    text = tweet['text']\n",
    "    if text not in tweets:\n",
    "        tweets.append(text)\n",
    "        \n",
    "for t in tweets:\n",
    "    print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE THIS: just for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " \"RT @levittownnow: Would you be interested in a hard cover or soft cover book featuring more than 100 curated photos from Pennsbury High Sch\\u2026\",\n",
      " \"Would you be interested in a hard cover or soft cover book featuring more than 100 curated photos from Pennsbury Hi\\u2026 https://t.co/BWJKRFex5J\",\n",
      " \"RT @WBCBNews: Pennsbury High School\\u2019s senior prom was back and better than ever Saturday night.\\n\\nhttps://t.co/ZQ91aCDGrw\",\n",
      " \"Pennsbury High School\\u2019s senior prom was back and better than ever Saturday night.\\n\\nhttps://t.co/ZQ91aCDGrw\",\n",
      " \"RT @thermoHailine: Had a great time talking to the Pennsbury High School seniors about the ocean and climate change (and was only mistaken\\u2026\"\n",
      "]\n",
      "[\n",
      " \"levittownnow\",\n",
      " \"WBCBNews\",\n",
      " \"thermoHailine\",\n",
      " \"DJPaulyD\",\n",
      " \"DJPaulyD\"\n",
      "]\n",
      "[\n",
      " \"Pennsbury\",\n",
      " \"Pennsbury\",\n",
      " \"BestPromInAmerica\",\n",
      " \"prom2019\",\n",
      " \"BestPromInAmerica\"\n",
      "]\n",
      "[\n",
      " \"RT\",\n",
      " \"@levittownnow:\",\n",
      " \"Would\",\n",
      " \"you\",\n",
      " \"be\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "status_texts = [ status['text']\n",
    "                 for status in results ]\n",
    "\n",
    "screen_names = [ user_mention['screen_name']\n",
    "                 for status in results\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "hashtags = [ hashtag['text']\n",
    "             for status in results\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w\n",
    "          for t in status_texts\n",
    "              for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print(json.dumps(status_texts[0:5], indent=1))\n",
    "print(json.dumps(screen_names[0:5], indent=1))\n",
    "print(json.dumps(hashtags[0:5], indent=1))\n",
    "print(json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: run sentiment analysis on the tweets included in the hashtags\n",
    "\n",
    "***Cells below oar from Week3a-TwitterMining-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/elijahcoggins/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarity Ranking of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.904 : \"Nicki is ready for her Sr Prom! Just gorgeous!have fun, Nicki! I love you so much! #pennsburyprom2019 @ Pennsbury H… https://t.co/dn9kUpaLBq\"\n",
      "-0.625 : \"before vs. after the desiigner disaster @ Pennsbury High School https://t.co/RlRKSpAK4T\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores = np.zeros(len(results))\n",
    "\n",
    "for i, t in enumerate(results):\n",
    "    # Extract the text portion of the tweet\n",
    "    text = t['text']\n",
    "\n",
    "    # Measure the polarity of the tweet\n",
    "    polarity = analyzer.polarity_scores(text)\n",
    "\n",
    "    # Store the normalized, weighted composite score\n",
    "    scores[i] = polarity['compound']\n",
    "\n",
    "most_positive = np.argmax(scores)\n",
    "most_negative = np.argmin(scores)\n",
    "\n",
    "print('{0:6.3f} : \"{1}\"'.format(scores[most_positive], results[most_positive]['text']))\n",
    "\n",
    "print('{0:6.3f} : \"{1}\"'.format(scores[most_negative], results[most_negative]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Create word maps of the words in the tweets that cross the threashhold\n",
    "\n",
    "***Cells below are from Week2-TwitterMining workbook and have not been formated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3a28a22b0ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecolor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import wordcloud\n",
    "words\n",
    "wc = wordcloud.WordCloud(stopwords=['https','RT','CO'])\n",
    "clean_string = ','.join(words)\n",
    "wc.generate(clean_string)\n",
    "plt.imshow(wc.recolor( random_state=3))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Create a network of the users bc of their followers/who they follow\n",
    "\n",
    "-would have to account for users who have tweeted multiple times, should we do this when saving the sentiment analysis/user/tweet so users can have multiple tweets or would we sort through when creating the network to find users with multiple tweets\n",
    "\n",
    "***Cells below are from Networkx_twitter and have not been formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/friends/list.PyTennessee.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4b5e0885b653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/friends/list.PyTennessee.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/friends/list.PyTennessee.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open('./data/friends/list.PyTennessee.json')\n",
    "\n",
    "data = json.load(f)\n",
    "pairs = []\n",
    "\n",
    "for user in data['users']:\n",
    "    pairs.append(('PyTennessee', str(user['screen_name'])))\n",
    "\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f57023fb33e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/friend_relationships/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpair_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "# Because the relationship data is split across files, we need to\n",
    "# walk through all of them to get the data.\n",
    "import os\n",
    "\n",
    "for (dir_path, dir_names, file_names) in os.walk('./data/friend_relationships/'):\n",
    "    files = file_names\n",
    "    \n",
    "for file_name in files:\n",
    "    with open('./data/friend_relationships/' + file_name) as p:\n",
    "        pair_data = json.load(p)\n",
    "        for k in pair_data.keys():\n",
    "            twitter_pair = k.split()\n",
    "            if pair_data[k]['relationship']['source']['following'] is True:\n",
    "                pairs.append((str(twitter_pair[0]), str(twitter_pair[1])))\n",
    "            elif pair_data[k]['relationship']['source']['followed_by'] is True:\n",
    "                pairs.append((str(twitter_pair[1]), str(twitter_pair[0])))\n",
    "                \n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import operator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an undirected graph. What's going on?\n",
    "g = nx.Graph()\n",
    "g.add_edges_from(pairs)\n",
    "nx.draw_networkx(g,font_size=6)\n",
    "plt.axis('off')\n",
    "plt.savefig('undirected.png', dpi=500)\n",
    "# How connected is the network?\n",
    "# Very connected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: if you want to sort a dictionary to easily \n",
    "# find the highest and lowest values, use this function \n",
    "# on the output of the centrality measures like degree_centrality():\n",
    "\n",
    "import operator\n",
    "\n",
    "def centrality_sort(centrality_dict):\n",
    "    return sorted(centrality_dict.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# ex. degree_sorted = centrality_sort(degree_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest degree centrality?\n",
    "degree_centrality = nx.degree_centrality(g)\n",
    "degree_sorted = centrality_sort(degree_centrality)\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest degree:', degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest degree:', degree_sorted[:5])\n",
    "print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest betweenness centrality?\n",
    "betweenness = nx.betweenness_centrality(g)\n",
    "betweenness_sorted = centrality_sort(betweenness)\n",
    "print ('-------------Betweenness Centrality-------------')\n",
    "print ('Highest betweenness:', betweenness_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest betweenness:', betweenness_sorted[:5])\n",
    "print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest closeness centrality?\n",
    "closeness = nx.closeness_centrality(g)\n",
    "closeness_sorted = centrality_sort(closeness)\n",
    "print ('-------------Closeness Centrality-------------')\n",
    "print ('Highest closeness:', closeness_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest closeness:', closeness_sorted[:5])\n",
    "\n",
    "# At the end, discuss these questions more in-depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at subsections of the graph.\n",
    "\n",
    "# Top 20 highest in-degree centrality scores:\n",
    "highest_degree = [node[0] for node in degree_sorted[-20:]]\n",
    "sub = g.subgraph(highest_degree)\n",
    "nx.draw_networkx(sub)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_degree = [node[0] for node in degree_sorted[:20]]\n",
    "subl = g.subgraph(lowest_degree)\n",
    "nx.draw_networkx(subl)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed graph\n",
    "\n",
    "d = nx.DiGraph()\n",
    "\n",
    "d.add_edges_from(pairs)\n",
    "nx.draw_networkx(d)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some degree centrality measures for directed graphs:\n",
    "# in_degree_centrality(): number of incoming connections (number of people following you)\n",
    "# out_degree_centrality(): number of outgoing connections (number of people you follow)\n",
    "in_degree_centrality = nx.in_degree_centrality(d)\n",
    "in_degree_sorted = sorted(in_degree_centrality.items(), key=operator.itemgetter(1))\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest in degree:', in_degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest in degree:', in_degree_sorted[:5])\n",
    "print ('\\n')\n",
    "\n",
    "out_degree_centrality = nx.out_degree_centrality(d)\n",
    "out_degree_sorted = sorted(out_degree_centrality.items(), key=operator.itemgetter(1))\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest out degree:', out_degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest out degree:', out_degree_sorted[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at subsections of the graph. Just like we did above.\n",
    "\n",
    "# Top 20 highest in-degree centrality scores:\n",
    "highest_in_degree = [node[0] for node in in_degree_sorted[-20:]]\n",
    "subin = d.subgraph(highest_in_degree)\n",
    "nx.draw_networkx(subin)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 highest out-degree centrality scores:\n",
    "highest_out_degree = [node[0] for node in out_degree_sorted[-20:]]\n",
    "subin = d.subgraph(highest_out_degree)\n",
    "nx.draw_networkx(subin)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# we need this 'magic' command to draw graphs inline\n",
    "%matplotlib inline  \n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "# let's attach a size attribute to each node to describe how big we want the node to be\n",
    "g.add_node(1, size= 800)\n",
    "g.add_node(2, size= 200)\n",
    "g.add_node(3, size= 200)\n",
    "g.add_node(4, size= 200)\n",
    "g.add_node(5, size= 200)\n",
    "\n",
    "g.add_edge(1,2, thickness= 20)\n",
    "g.add_edge(1,3, thickness= 20)\n",
    "g.add_edge(1,4, thickness= 20)\n",
    "g.add_edge(1,5, thickness= 20)\n",
    "g.add_edge(2,3, thickness= 5)\n",
    "g.add_edge(3,4, thickness= 5)\n",
    "g.add_edge(4,5, thickness= 5)\n",
    "g.add_edge(5,2, thickness= 5)\n",
    "\n",
    "# let's iterate through the nodes and edges and extract the list of node & edge sizes\n",
    "node_size = [attribs['size'] for (node, attribs) in g.nodes(data=True)]\n",
    "edge_thickness = [attribs['thickness'] for (v_from, v_to, attribs) in g.edges(data=True)]\n",
    "\n",
    "LIGHT_BLUE = '#A0CBE2'\n",
    "\n",
    "nx.draw_networkx(g, \n",
    "    node_size = node_size,   # node_size can either take a single value (where all nodes will be size N),\n",
    "                             # or a list of values, where Nth list value will be the size for the Nth node\n",
    "    width = edge_thickness,  # similarly, the Nth value corresponds to the width for edge N\n",
    "    node_color = LIGHT_BLUE,\n",
    "    edge_color = LIGHT_BLUE,\n",
    "    font_size = 15,\n",
    "    with_labels = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_txt = './data/retweets.txt'\n",
    "G = nx.read_edgelist(edgelist_txt, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYOUTS = {\n",
    "    'circular': nx.circular_layout,\n",
    "    'fr': nx.fruchterman_reingold_layout,\n",
    "    'random': nx.random_layout,\n",
    "    'shell': nx.shell_layout,\n",
    "    'spectral': nx.spectral_layout,\n",
    "    'spring': nx.spring_layout\n",
    "}\n",
    "\n",
    "def save_layout(G, layout_name):\n",
    "    elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] >1.5]\n",
    "    esmall=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] <=1.5]\n",
    "    nlarge=[n for n in G.nodes() if n in ['PyTennessee']]\n",
    "    pos=LAYOUTS[layout_name](G) # positions for all nodes\n",
    "\n",
    "    print (nlarge)\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G,pos,nodelist=nlarge,node_size=1)\n",
    "\n",
    "    # edges\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=elarge, width=1)\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=esmall, width=1,alpha=0.5,edge_color='#cccccc')\n",
    "\n",
    "    # labelsM\n",
    "    labels={}\n",
    "    labels['PyTennessee']='PyTennessee'\n",
    "    nx.draw_networkx_labels(G,pos,labels,font_size=6)\n",
    "    #nx.draw_networkx_labels(G,pos,nodelist=nlarge,font_size=6,font_family='sans-serif')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(layout_name + '.png', dpi=500)\n",
    "\n",
    "save_layout(G, 'spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Visually identify the users that have tweeted a negative tweet or a positive tweet with red or green"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
