{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Twitter Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<twitter.api.Twitter object at 0x108385d30>\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "import json\n",
    "\n",
    "def oauth_login():\n",
    "    # XXX: Go to  to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://developer.twitter.com/en/docs/basics/authentication/overview/oauth\n",
    "    # for more information on Twitter's OAuth implementation.\n",
    "    \n",
    "    CONSUMER_KEY = 'ldJnCzzuCM3Col1pMcmSyVaDT'\n",
    "    CONSUMER_SECRET = 'RWCNFVehlhAHzarhuAJA7vjSfoQx1FB96HLY7VR2U1d6mDj31w'\n",
    "    OAUTH_TOKEN = '1112769260818972672-FFlTewXnvPcbcJAM04vYqk8DPxjYcG'\n",
    "    OAUTH_TOKEN_SECRET = 'D5EuZziAVsW26tHuZCGYRw2uiKPdfdUgaaIszGatEsdB6'\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "# Sample usage\n",
    "twitter_api = oauth_login()    \n",
    "\n",
    "print(twitter_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"created_at\": \"Tue Jun 04 20:25:40 +0000 2019\",\n",
      " \"id\": 1136006124111564802,\n",
      " \"id_str\": \"1136006124111564802\",\n",
      " \"text\": \"RT @levittownnow: Would you be interested in a hard cover or soft cover book featuring more than 100 curated photos from Pennsbury High Sch\\u2026\",\n",
      " \"truncated\": false,\n",
      " \"entities\": {\n",
      "  \"hashtags\": [],\n",
      "  \"symbols\": [],\n",
      "  \"user_mentions\": [\n",
      "   {\n",
      "    \"screen_name\": \"levittownnow\",\n",
      "    \"name\": \"LevittownNow.com\",\n",
      "    \"id\": 1231897549,\n",
      "    \"id_str\": \"1231897549\",\n",
      "    \"indices\": [\n",
      "     3,\n",
      "     16\n",
      "    ]\n",
      "   }\n",
      "  ],\n",
      "  \"urls\": []\n",
      " },\n",
      " \"metadata\": {\n",
      "  \"iso_language_code\": \"en\",\n",
      "  \"result_type\": \"recent\"\n",
      " },\n",
      " \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\",\n",
      " \"in_reply_to_status_id\": null,\n",
      " \"in_reply_to_status_id_str\": null,\n",
      " \"in_reply_to_user_id\": null,\n",
      " \"in_reply_to_user_id_str\": null,\n",
      " \"in_reply_to_screen_name\": null,\n",
      " \"user\": {\n",
      "  \"id\": 175146866,\n",
      "  \"id_str\": \"175146866\",\n",
      "  \"name\": \"Tom Sofield\",\n",
      "  \"screen_name\": \"BuxMontNews\",\n",
      "  \"location\": \"Bucks County\",\n",
      "  \"description\": \"Publisher/Editor, @LevittownNow & @NewtownPaNow | #LocalNewsMatters | Byline has appeared @patchtweet, @montgomerymedia & more\",\n",
      "  \"url\": \"https://t.co/cbhIggmHLZ\",\n",
      "  \"entities\": {\n",
      "   \"url\": {\n",
      "    \"urls\": [\n",
      "     {\n",
      "      \"url\": \"https://t.co/cbhIggmHLZ\",\n",
      "      \"expanded_url\": \"http://levittownnow.com\",\n",
      "      \"display_url\": \"levittownnow.com\",\n",
      "      \"indices\": [\n",
      "       0,\n",
      "       23\n",
      "      ]\n",
      "     }\n",
      "    ]\n",
      "   },\n",
      "   \"description\": {\n",
      "    \"urls\": []\n",
      "   }\n",
      "  },\n",
      "  \"protected\": false,\n",
      "  \"followers_count\": 4366,\n",
      "  \"friends_count\": 3291,\n",
      "  \"listed_count\": 127,\n",
      "  \"created_at\": \"Thu Aug 05 19:19:41 +0000 2010\",\n",
      "  \"favourites_count\": 7749,\n",
      "  \"utc_offset\": null,\n",
      "  \"time_zone\": null,\n",
      "  \"geo_enabled\": true,\n",
      "  \"verified\": false,\n",
      "  \"statuses_count\": 44675,\n",
      "  \"lang\": \"en\",\n",
      "  \"contributors_enabled\": false,\n",
      "  \"is_translator\": false,\n",
      "  \"is_translation_enabled\": false,\n",
      "  \"profile_background_color\": \"C0DEED\",\n",
      "  \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "  \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "  \"profile_background_tile\": false,\n",
      "  \"profile_image_url\": \"http://pbs.twimg.com/profile_images/1093986525476503552/YJCU5xC6_normal.jpg\",\n",
      "  \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/1093986525476503552/YJCU5xC6_normal.jpg\",\n",
      "  \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/175146866/1398687650\",\n",
      "  \"profile_link_color\": \"1DA1F2\",\n",
      "  \"profile_sidebar_border_color\": \"C0DEED\",\n",
      "  \"profile_sidebar_fill_color\": \"DDEEF6\",\n",
      "  \"profile_text_color\": \"333333\",\n",
      "  \"profile_use_background_image\": true,\n",
      "  \"has_extended_profile\": true,\n",
      "  \"default_profile\": true,\n",
      "  \"default_profile_image\": false,\n",
      "  \"following\": false,\n",
      "  \"follow_request_sent\": false,\n",
      "  \"notifications\": false,\n",
      "  \"translator_type\": \"none\"\n",
      " },\n",
      " \"geo\": null,\n",
      " \"coordinates\": null,\n",
      " \"place\": null,\n",
      " \"contributors\": null,\n",
      " \"retweeted_status\": {\n",
      "  \"created_at\": \"Tue Jun 04 19:52:18 +0000 2019\",\n",
      "  \"id\": 1135997726641008642,\n",
      "  \"id_str\": \"1135997726641008642\",\n",
      "  \"text\": \"Would you be interested in a hard cover or soft cover book featuring more than 100 curated photos from Pennsbury Hi\\u2026 https://t.co/BWJKRFex5J\",\n",
      "  \"truncated\": true,\n",
      "  \"entities\": {\n",
      "   \"hashtags\": [],\n",
      "   \"symbols\": [],\n",
      "   \"user_mentions\": [],\n",
      "   \"urls\": [\n",
      "    {\n",
      "     \"url\": \"https://t.co/BWJKRFex5J\",\n",
      "     \"expanded_url\": \"https://twitter.com/i/web/status/1135997726641008642\",\n",
      "     \"display_url\": \"twitter.com/i/web/status/1\\u2026\",\n",
      "     \"indices\": [\n",
      "      117,\n",
      "      140\n",
      "     ]\n",
      "    }\n",
      "   ]\n",
      "  },\n",
      "  \"metadata\": {\n",
      "   \"iso_language_code\": \"en\",\n",
      "   \"result_type\": \"recent\"\n",
      "  },\n",
      "  \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\",\n",
      "  \"in_reply_to_status_id\": null,\n",
      "  \"in_reply_to_status_id_str\": null,\n",
      "  \"in_reply_to_user_id\": null,\n",
      "  \"in_reply_to_user_id_str\": null,\n",
      "  \"in_reply_to_screen_name\": null,\n",
      "  \"user\": {\n",
      "   \"id\": 1231897549,\n",
      "   \"id_str\": \"1231897549\",\n",
      "   \"name\": \"LevittownNow.com\",\n",
      "   \"screen_name\": \"levittownnow\",\n",
      "   \"location\": \"Levittown, Pa.\",\n",
      "   \"description\": \"Local news & information for over 150,000 people living in and around Levittown, #BucksCounty.  news@levittownnow.com\",\n",
      "   \"url\": \"http://t.co/cbhIggEiDx\",\n",
      "   \"entities\": {\n",
      "    \"url\": {\n",
      "     \"urls\": [\n",
      "      {\n",
      "       \"url\": \"http://t.co/cbhIggEiDx\",\n",
      "       \"expanded_url\": \"http://levittownnow.com\",\n",
      "       \"display_url\": \"levittownnow.com\",\n",
      "       \"indices\": [\n",
      "        0,\n",
      "        22\n",
      "       ]\n",
      "      }\n",
      "     ]\n",
      "    },\n",
      "    \"description\": {\n",
      "     \"urls\": []\n",
      "    }\n",
      "   },\n",
      "   \"protected\": false,\n",
      "   \"followers_count\": 5053,\n",
      "   \"friends_count\": 1234,\n",
      "   \"listed_count\": 55,\n",
      "   \"created_at\": \"Fri Mar 01 23:00:30 +0000 2013\",\n",
      "   \"favourites_count\": 1925,\n",
      "   \"utc_offset\": null,\n",
      "   \"time_zone\": null,\n",
      "   \"geo_enabled\": false,\n",
      "   \"verified\": false,\n",
      "   \"statuses_count\": 22022,\n",
      "   \"lang\": \"en\",\n",
      "   \"contributors_enabled\": false,\n",
      "   \"is_translator\": false,\n",
      "   \"is_translation_enabled\": false,\n",
      "   \"profile_background_color\": \"C0DEED\",\n",
      "   \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "   \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "   \"profile_background_tile\": false,\n",
      "   \"profile_image_url\": \"http://pbs.twimg.com/profile_images/1066381124706811904/6TE0RDaJ_normal.jpg\",\n",
      "   \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/1066381124706811904/6TE0RDaJ_normal.jpg\",\n",
      "   \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/1231897549/1558449760\",\n",
      "   \"profile_link_color\": \"1DA1F2\",\n",
      "   \"profile_sidebar_border_color\": \"C0DEED\",\n",
      "   \"profile_sidebar_fill_color\": \"DDEEF6\",\n",
      "   \"profile_text_color\": \"333333\",\n",
      "   \"profile_use_background_image\": true,\n",
      "   \"has_extended_profile\": false,\n",
      "   \"default_profile\": true,\n",
      "   \"default_profile_image\": false,\n",
      "   \"following\": false,\n",
      "   \"follow_request_sent\": false,\n",
      "   \"notifications\": false,\n",
      "   \"translator_type\": \"none\"\n",
      "  },\n",
      "  \"geo\": null,\n",
      "  \"coordinates\": null,\n",
      "  \"place\": null,\n",
      "  \"contributors\": null,\n",
      "  \"is_quote_status\": false,\n",
      "  \"retweet_count\": 1,\n",
      "  \"favorite_count\": 0,\n",
      "  \"favorited\": false,\n",
      "  \"retweeted\": false,\n",
      "  \"possibly_sensitive\": false,\n",
      "  \"lang\": \"en\"\n",
      " },\n",
      " \"is_quote_status\": false,\n",
      " \"retweet_count\": 1,\n",
      " \"favorite_count\": 0,\n",
      " \"favorited\": false,\n",
      " \"retweeted\": false,\n",
      " \"lang\": \"en\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://developer.twitter.com/en/docs/basics/rate-limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    # Read this https://developer.twitter.com/en/docs/tweets/timelines/guides/working-with-timelines to understand why we use \n",
    "    # max_id to populate tweets\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "q = \"Pennsbury High School\"\n",
    "results = twitter_search(twitter_api, q, max_results=1000)\n",
    "        \n",
    "# Show one sample search result by slicing the list...\n",
    "print(json.dumps(results[0], indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering the public timeline for track=Pennsbury\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2ed1f2095c07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# See https://developer.twitter.com/en/docs/tweets/filter-realtime/guides/basic-stream-parameters.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatuses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/twitter/stream.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# Decode all the things:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mHangup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/twitter/stream.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mready_to_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mready_to_read\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finding topics of interest by using the filtering capabilities it offers.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Query terms\n",
    "\n",
    "q = 'Pennsbury' # Comma-separated list of terms\n",
    "\n",
    "print('Filtering the public timeline for track={0}'.format(q), file=sys.stderr)\n",
    "sys.stderr.flush()\n",
    "\n",
    "# Returns an instance of twitter.Twitter\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# Reference the self.auth parameter\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "\n",
    "# See https://developer.twitter.com/en/docs/tweets/filter-realtime/guides/basic-stream-parameters.html\n",
    "stream = twitter_stream.statuses.filter(track=q)\n",
    "for tweet in stream:\n",
    "    print(tweet['text'])\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " \"RT @levittownnow: Would you be interested in a hard cover or soft cover book featuring more than 100 curated photos from Pennsbury High Sch\\u2026\",\n",
      " \"Would you be interested in a hard cover or soft cover book featuring more than 100 curated photos from Pennsbury Hi\\u2026 https://t.co/BWJKRFex5J\",\n",
      " \"RT @WBCBNews: Pennsbury High School\\u2019s senior prom was back and better than ever Saturday night.\\n\\nhttps://t.co/ZQ91aCDGrw\",\n",
      " \"Pennsbury High School\\u2019s senior prom was back and better than ever Saturday night.\\n\\nhttps://t.co/ZQ91aCDGrw\",\n",
      " \"RT @thermoHailine: Had a great time talking to the Pennsbury High School seniors about the ocean and climate change (and was only mistaken\\u2026\"\n",
      "]\n",
      "[\n",
      " \"levittownnow\",\n",
      " \"WBCBNews\",\n",
      " \"thermoHailine\",\n",
      " \"DJPaulyD\",\n",
      " \"DJPaulyD\"\n",
      "]\n",
      "[\n",
      " \"Pennsbury\",\n",
      " \"Pennsbury\",\n",
      " \"BestPromInAmerica\",\n",
      " \"prom2019\",\n",
      " \"BestPromInAmerica\"\n",
      "]\n",
      "[\n",
      " \"RT\",\n",
      " \"@levittownnow:\",\n",
      " \"Would\",\n",
      " \"you\",\n",
      " \"be\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "status_texts = [ status['text']\n",
    "                 for status in results ]\n",
    "\n",
    "screen_names = [ user_mention['screen_name']\n",
    "                 for status in results\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "hashtags = [ hashtag['text']\n",
    "             for status in results\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w\n",
    "          for t in status_texts\n",
    "              for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print(json.dumps(status_texts[0:5], indent=1))\n",
    "print(json.dumps(screen_names[0:5], indent=1))\n",
    "print(json.dumps(hashtags[0:5], indent=1))\n",
    "print(json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: run sentiment analysis on the tweets included in the hashtags\n",
    "\n",
    "***Cells below oar from Week3a-TwitterMining-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/elijahcoggins/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_api = oauth_login()\n",
    "\n",
    "# Reference the self.auth parameter\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "iterator = twitter_stream.statuses.sample()\n",
    "\n",
    "tweets = []\n",
    "for tweet in iterator:\n",
    "    try:\n",
    "        if tweet['lang'] == 'en':\n",
    "            tweets.append(tweet)\n",
    "    except:\n",
    "        pass\n",
    "    if len(tweets) == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.949 : \"RT @ithinkthatway: True love requires faith, trust and loyalty. Not chocolate, flowers and expensive gifts\"\n",
      "-0.822 : \"It hurts so bad ðŸ’”\"\n"
     ]
    }
   ],
   "source": [
    "scores = np.zeros(len(tweets))\n",
    "\n",
    "for i, t in enumerate(tweets):\n",
    "    # Extract the text portion of the tweet\n",
    "    text = t['text']\n",
    "    \n",
    "    # Measure the polarity of the tweet\n",
    "    polarity = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Store the normalized, weighted composite score\n",
    "    scores[i] = polarity['compound']\n",
    "\n",
    "most_positive = np.argmax(scores)\n",
    "most_negative = np.argmin(scores)\n",
    "\n",
    "print('{0:6.3f} : \"{1}\"'.format(scores[most_positive], tweets[most_positive]['text']))\n",
    "\n",
    "print('{0:6.3f} : \"{1}\"'.format(scores[most_negative], tweets[most_negative]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Store user, tweet, and sentiment analysis score above () and below () in a doc or database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Create word maps of the words in the tweets that cross the threashhold\n",
    "\n",
    "***Cells below are from Week2-TwitterMining workbook and have not been formated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3a28a22b0ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecolor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import wordcloud\n",
    "words\n",
    "wc = wordcloud.WordCloud(stopwords=['https','RT','CO'])\n",
    "clean_string = ','.join(words)\n",
    "wc.generate(clean_string)\n",
    "plt.imshow(wc.recolor( random_state=3))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Create a network of the users bc of their followers/who they follow\n",
    "\n",
    "-would have to account for users who have tweeted multiple times, should we do this when saving the sentiment analysis/user/tweet so users can have multiple tweets or would we sort through when creating the network to find users with multiple tweets\n",
    "\n",
    "***Cells below are from Networkx_twitter and have not been formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/friends/list.PyTennessee.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4b5e0885b653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/friends/list.PyTennessee.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/friends/list.PyTennessee.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open('./data/friends/list.PyTennessee.json')\n",
    "\n",
    "data = json.load(f)\n",
    "pairs = []\n",
    "\n",
    "for user in data['users']:\n",
    "    pairs.append(('PyTennessee', str(user['screen_name'])))\n",
    "\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f57023fb33e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/friend_relationships/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpair_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "# Because the relationship data is split across files, we need to\n",
    "# walk through all of them to get the data.\n",
    "import os\n",
    "\n",
    "for (dir_path, dir_names, file_names) in os.walk('./data/friend_relationships/'):\n",
    "    files = file_names\n",
    "    \n",
    "for file_name in files:\n",
    "    with open('./data/friend_relationships/' + file_name) as p:\n",
    "        pair_data = json.load(p)\n",
    "        for k in pair_data.keys():\n",
    "            twitter_pair = k.split()\n",
    "            if pair_data[k]['relationship']['source']['following'] is True:\n",
    "                pairs.append((str(twitter_pair[0]), str(twitter_pair[1])))\n",
    "            elif pair_data[k]['relationship']['source']['followed_by'] is True:\n",
    "                pairs.append((str(twitter_pair[1]), str(twitter_pair[0])))\n",
    "                \n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import operator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an undirected graph. What's going on?\n",
    "g = nx.Graph()\n",
    "g.add_edges_from(pairs)\n",
    "nx.draw_networkx(g,font_size=6)\n",
    "plt.axis('off')\n",
    "plt.savefig('undirected.png', dpi=500)\n",
    "# How connected is the network?\n",
    "# Very connected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: if you want to sort a dictionary to easily \n",
    "# find the highest and lowest values, use this function \n",
    "# on the output of the centrality measures like degree_centrality():\n",
    "\n",
    "import operator\n",
    "\n",
    "def centrality_sort(centrality_dict):\n",
    "    return sorted(centrality_dict.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# ex. degree_sorted = centrality_sort(degree_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest degree centrality?\n",
    "degree_centrality = nx.degree_centrality(g)\n",
    "degree_sorted = centrality_sort(degree_centrality)\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest degree:', degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest degree:', degree_sorted[:5])\n",
    "print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest betweenness centrality?\n",
    "betweenness = nx.betweenness_centrality(g)\n",
    "betweenness_sorted = centrality_sort(betweenness)\n",
    "print ('-------------Betweenness Centrality-------------')\n",
    "print ('Highest betweenness:', betweenness_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest betweenness:', betweenness_sorted[:5])\n",
    "print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality: which nodes have the highest/lowest closeness centrality?\n",
    "closeness = nx.closeness_centrality(g)\n",
    "closeness_sorted = centrality_sort(closeness)\n",
    "print ('-------------Closeness Centrality-------------')\n",
    "print ('Highest closeness:', closeness_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest closeness:', closeness_sorted[:5])\n",
    "\n",
    "# At the end, discuss these questions more in-depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at subsections of the graph.\n",
    "\n",
    "# Top 20 highest in-degree centrality scores:\n",
    "highest_degree = [node[0] for node in degree_sorted[-20:]]\n",
    "sub = g.subgraph(highest_degree)\n",
    "nx.draw_networkx(sub)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_degree = [node[0] for node in degree_sorted[:20]]\n",
    "subl = g.subgraph(lowest_degree)\n",
    "nx.draw_networkx(subl)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed graph\n",
    "\n",
    "d = nx.DiGraph()\n",
    "\n",
    "d.add_edges_from(pairs)\n",
    "nx.draw_networkx(d)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some degree centrality measures for directed graphs:\n",
    "# in_degree_centrality(): number of incoming connections (number of people following you)\n",
    "# out_degree_centrality(): number of outgoing connections (number of people you follow)\n",
    "in_degree_centrality = nx.in_degree_centrality(d)\n",
    "in_degree_sorted = sorted(in_degree_centrality.items(), key=operator.itemgetter(1))\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest in degree:', in_degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest in degree:', in_degree_sorted[:5])\n",
    "print ('\\n')\n",
    "\n",
    "out_degree_centrality = nx.out_degree_centrality(d)\n",
    "out_degree_sorted = sorted(out_degree_centrality.items(), key=operator.itemgetter(1))\n",
    "print ('-------------Degree Centrality-------------')\n",
    "print ('Highest out degree:', out_degree_sorted[-5:])\n",
    "print ('\\n')\n",
    "print ('Lowest out degree:', out_degree_sorted[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at subsections of the graph. Just like we did above.\n",
    "\n",
    "# Top 20 highest in-degree centrality scores:\n",
    "highest_in_degree = [node[0] for node in in_degree_sorted[-20:]]\n",
    "subin = d.subgraph(highest_in_degree)\n",
    "nx.draw_networkx(subin)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 highest out-degree centrality scores:\n",
    "highest_out_degree = [node[0] for node in out_degree_sorted[-20:]]\n",
    "subin = d.subgraph(highest_out_degree)\n",
    "nx.draw_networkx(subin)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# we need this 'magic' command to draw graphs inline\n",
    "%matplotlib inline  \n",
    "\n",
    "g = nx.Graph()\n",
    "\n",
    "# let's attach a size attribute to each node to describe how big we want the node to be\n",
    "g.add_node(1, size= 800)\n",
    "g.add_node(2, size= 200)\n",
    "g.add_node(3, size= 200)\n",
    "g.add_node(4, size= 200)\n",
    "g.add_node(5, size= 200)\n",
    "\n",
    "g.add_edge(1,2, thickness= 20)\n",
    "g.add_edge(1,3, thickness= 20)\n",
    "g.add_edge(1,4, thickness= 20)\n",
    "g.add_edge(1,5, thickness= 20)\n",
    "g.add_edge(2,3, thickness= 5)\n",
    "g.add_edge(3,4, thickness= 5)\n",
    "g.add_edge(4,5, thickness= 5)\n",
    "g.add_edge(5,2, thickness= 5)\n",
    "\n",
    "# let's iterate through the nodes and edges and extract the list of node & edge sizes\n",
    "node_size = [attribs['size'] for (node, attribs) in g.nodes(data=True)]\n",
    "edge_thickness = [attribs['thickness'] for (v_from, v_to, attribs) in g.edges(data=True)]\n",
    "\n",
    "LIGHT_BLUE = '#A0CBE2'\n",
    "\n",
    "nx.draw_networkx(g, \n",
    "    node_size = node_size,   # node_size can either take a single value (where all nodes will be size N),\n",
    "                             # or a list of values, where Nth list value will be the size for the Nth node\n",
    "    width = edge_thickness,  # similarly, the Nth value corresponds to the width for edge N\n",
    "    node_color = LIGHT_BLUE,\n",
    "    edge_color = LIGHT_BLUE,\n",
    "    font_size = 15,\n",
    "    with_labels = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_txt = './data/retweets.txt'\n",
    "G = nx.read_edgelist(edgelist_txt, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYOUTS = {\n",
    "    'circular': nx.circular_layout,\n",
    "    'fr': nx.fruchterman_reingold_layout,\n",
    "    'random': nx.random_layout,\n",
    "    'shell': nx.shell_layout,\n",
    "    'spectral': nx.spectral_layout,\n",
    "    'spring': nx.spring_layout\n",
    "}\n",
    "\n",
    "def save_layout(G, layout_name):\n",
    "    elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] >1.5]\n",
    "    esmall=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] <=1.5]\n",
    "    nlarge=[n for n in G.nodes() if n in ['PyTennessee']]\n",
    "    pos=LAYOUTS[layout_name](G) # positions for all nodes\n",
    "\n",
    "    print (nlarge)\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G,pos,nodelist=nlarge,node_size=1)\n",
    "\n",
    "    # edges\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=elarge, width=1)\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=esmall, width=1,alpha=0.5,edge_color='#cccccc')\n",
    "\n",
    "    # labelsM\n",
    "    labels={}\n",
    "    labels['PyTennessee']='PyTennessee'\n",
    "    nx.draw_networkx_labels(G,pos,labels,font_size=6)\n",
    "    #nx.draw_networkx_labels(G,pos,nodelist=nlarge,font_size=6,font_family='sans-serif')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(layout_name + '.png', dpi=500)\n",
    "\n",
    "save_layout(G, 'spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Visually identify the users that have tweeted a negative tweet or a positive tweet with red or green"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
